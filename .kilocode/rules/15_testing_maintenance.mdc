---
description: Testing maintenance rules for c0r.AI project
alwaysApply: true
---

# Testing Maintenance Rules

## Core Principle
Every code change MUST maintain or improve test coverage. Tests are living documentation and safety nets for refactoring.

## Test Organization
```
tests/
├── unit/                       # Unit tests (fast, isolated)
├── integration/                # Integration tests (services interaction)
├── e2e/                       # End-to-end tests (full user flows)
├── fixtures/                  # Test data and fixtures
└── conftest.py               # Pytest configuration and fixtures
```

## Test Requirements

### When Adding New Features
- Unit tests for all new functions/methods
- Integration tests for service interactions
- E2E tests for complete user workflows
- Mock external service dependencies
- Test error conditions and edge cases

### When Modifying Existing Code
- Update existing tests to match new behavior
- Add tests for new code paths
- Ensure all modified functions have test coverage
- Run full test suite to catch regressions

### When Fixing Bugs
- Write test that reproduces the bug first
- Verify test fails before fix
- Implement fix and verify test passes
- Add regression tests for similar scenarios

## Test Standards
- Use descriptive test names that explain what is being tested
- Follow Arrange-Act-Assert pattern
- Test both happy path and error conditions
- Use parametrized tests for multiple input scenarios
- Mock external dependencies properly

## Test Execution
```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=. --cov-report=html --cov-fail-under=85
```

## Checklist
Before each PR:
- [ ] All new code has corresponding tests
- [ ] All tests pass locally
- [ ] Test coverage meets minimum threshold (85%)
- [ ] No flaky or intermittent test failures
- [ ] Test data and fixtures updated if needed

Tests serve as executable documentation of system behavior.